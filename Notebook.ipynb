{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einleitung\n",
    "Schiffe versenken ist ein simples Spiel, welches von zwei Personen, oder einer Person und einem Computer gespielt werden kann. Ziel des Spiels ist es die von dem gegnerischen Spieler platzierten Schiffe zu versenken. Gewonnen hat der Spieler, welcher als Erstes alle Schiffe des Gegners versenkt hat. Durch den simplen Aufbau ist Schiffe versenken eins der ersten Spiele, die auf einem Computer gespielt werden konnten.\n",
    "\n",
    "<img src=\"Images/abbildung2.svg\" width=\"400\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "Reinforcement Learning ist eine bew√§hrte Methode, um einem Computersystem beizubringen, ein Spiel zu spielen und in diesem Spiel eigene Strategien zu entwickeln. Diese neu entwickelten Strategien erlauben es, in vielen F√§llen, dass das Computersystem einem Menschen in dem Spiel √ºberlegen ist. Ein Agent der jeden Zug zuf√§llig ausw√§hlt braucht, bei einem 10x10 Spielfeld mit der klassischen Anzahl von Schiffen, im Durchschnitt ~96 Sch√ºsse. Durch einen Hunt Algorithmus kann die Anzahl der Sch√ºsse auf ~65 Sch√ºsse reduziert werden [1]. Ein durch Reinforcement Learning trainierter Agent kann in der Lage sein ein Spiel in ~52 zu beenden [2]. Damit ist dieser besser als ein Menschlicher Spieler.\n",
    "\n",
    "Das Hauptkonzept von Reinforcement Learning wird in der Abbildung 1 dargestellt.  \n",
    "![abbildung1](./images/abbildung1.png)\n",
    "Figure 1: Reinforcement Learning Prozess  \n",
    "\n",
    "Ein Agent nimmt durch eine Action Einfluss auf die Umgebung. Im Kontext von Schiffe versenken bedeutet dies, dass ein Feld ausgew√§hlt wird, welches beschossen wird. Die Umgebung gibt dem Agenten dann auf zwei wegen Feedback. Zum Einen gibt die Umgebung Feedback zu dem neuen State der Umgebung also z. B. Treffer, versenkt oder nicht getroffen. Zum Anderen gibt die Umgebung dem Agenten einen Reward. √úber diesen Reward bekommt der Agent mitgeteilt, wie gut oder schlecht seine ausgew√§hlte Action war. Ziel des Agent ist es einen m√∂glichst hohen Reward zu bekommen, sprich m√∂glichst Effektiv das Spiel zu gewinnen. Das beschriebene Prinzip wird auch Markov-Decision-Process genannt. Der Markov-Decision-Process nimmt an, dass der aktuelle State einer Umgebung repr√§sentativ f√ºr alle States ist, die vorher in der Umgebung existiert haben.  \n",
    "\t* S: Alle m√∂glichen States\n",
    "\t* A: Alle m√∂glichen Actions\n",
    "\t* R: Reward Verteilung bei aktuellem State und ausgef√ºhrter Action (s, a)\n",
    "\t* P: Transition probability. Die Wahrscheinlichkeit welchen State die Umgebung, nach einer ausgef√ºhrten Action auf dem aktuellen State, besitzt.\n",
    "\t* …£: Discount factor. Hyperparameter, welcher von au√üen gesetzt werden kann. Beschreibt wie wichtig zuk√ºnftige rewards f√ºr den aktuellen State sind.\n",
    "\n",
    "Ziel ist es eine optimale Policy ùúã* zu finden, die angibt welche Aktion bei einem aktuellen State auszuf√ºhren ist.  \n",
    "\n",
    "$\\pi^*$=max($\\sum_{t<0} \\gamma^t r^t$)\n",
    "\n",
    "Mathematisch definiert ist diese Policy ùúã* durch das Maximum aus der Summe aller Rewards. Dabei wird der discount factor benutzt um zuk√ºnftige Rewards zu gewichten [1].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install stable-baselines==2.10.0 tensorflow==1.13.2 gym==0.17.2 numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Umgebung   \n",
    "Die Umgebung des Spiels Schiffe versenken, kann wie folgt beschrieben werden:  \n",
    "<img src=\"Images/tabelle1.png\" width=\"600\">  \n",
    "Die einzige Aktion, die der Agent innerhalb der Umgebung ben√∂tigt, ist der Beschuss eines Feldes. Dem Agenten stehen Percepts zur Verf√ºgung. Sie werden √ºber den Zustand jedes einzelnen Feldes zur Verf√ºgung gestellt. Ein Feld kann folgende Informationen enthalten:  \n",
    "![Tabelle2](./images/tabelle2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konfiguration\n",
    "Die Config Klasse nimmt die Spielkonfiguration entgegen. Die Konfiguration wird von der Umgebung √ºbernommen und ist die Grundlage, auf welcher der Agent das Spiel lernt. √Ñndern wir Attribute in der Konfiguration, so m√ºssen wir den Agenten auf diese neue Konfiguration und somit neue Umgebung trainieren. \n",
    "* board_size: F√ºr das Training ist eine board_size von 5x5 zu empfehlen. Bei 10x10 verl√§ngert sich das Training entsprechend\n",
    "* ships: Die Schiffe m√ºssen auf das Board passen. F√ºr das Training reichen auch wenige Schiffe\n",
    "* gap: Zu beachten ist hierbei, dass wenn eine Gap existieren soll, deutlich weniger Platz f√ºr die Schiffe zu Verf√ºgung steht\n",
    "* static_placement: Beim statischen placement √§ndert sich die Schiffposition nicht bei jedem Durchlauf. Es ist zu erwarten, dass die Agenten bei dieser Einstellung schnell Overfitten\n",
    "* binary_reward: Gibt an, ob der Agent lernen soll, valide Z√ºge zu spielen, d.h keine Felder doppelt beschie√üen, oder ob der Agent lernen soll, Schiffe m√∂glichst effektiv zu versenken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class representing a config object\n",
    "\"\"\"\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Constructor for a Config object\n",
    "    Arguments:\n",
    "    board_size = Number of fields in x and y direction.\n",
    "    ships = Array of Ships with length of ships. Example [2,2,3] Sets 2 Ships with length of 2, 1 ship with length 3\n",
    "    gap = Boolean: True => Ships have a gap of at least 1 water field between each other.\n",
    "    False => Ships can be placed side by side\n",
    "    static_placement => A static placement for ships over all iterations is used\n",
    "    binary_reward => The reward will be +1 for a valid action and -1 for an invalid action\n",
    "    to train an agent not to choose the same action multiple times.\n",
    "    \"\"\"\n",
    "    def __init__(self, board_size, ships, gap, static_placement, binary_reward):\n",
    "        self.board_size = board_size\n",
    "        self.ships = ships\n",
    "        self.gap = gap\n",
    "        self.static_placement = static_placement\n",
    "        self.binary_reward = binary_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Environment\n",
    "F√ºr die Umgebung wurde die OpenAI Gym Library verwendet. Diese Library liefert eine Struktur, mithilfe dessen es m√∂glich ist, entweder auf bereits existierende Umgebungen zuzugreifen (https://gym.openai.com/envs/#classic_control) oder eigene Umgebungen zu bauen. Das hei√üt, f√ºr den Anwendungsfall muss ein custom Environment geschrieben werden, welches das Schiffe versenken Spiel implementiert. Folgende Schritte sind daf√ºr notwendig:\n",
    "1. Die Definition des Environments\n",
    "2. L√∂schen eines alten registierten Environments \n",
    "3. Registieren des neuen Environments\n",
    "\n",
    "Die Klasse Ship repr√§sentiert ein einzelnens Schiff. Es beinhaltet die Startkoordinaten, die Ausrichtung und die L√§nge des Schiffs. In dieser Klasse wird √ºberpr√ºft, ob ein Schiff getroffen wurde und ob es versunken wurde.\n",
    "\n",
    "Die Umgebung wird in der BattleshipsEnv Klasse beschrieben. F√ºr die Definition eines eigenen Enviroments mit Hilfe von OpenAI werden folgende Methoden ben√∂tigt: \n",
    "1. Constructor: initalisiert alle ben√∂tigten Variablen.\n",
    "2. Step: F√ºhrt eine Aktion auf der aktuellen Umgebung aus.\n",
    "3. Reset: Setzt die Umgebung wieder auf den Startzustand.\n",
    "4. Render: Zeichnet die Umgebung auf der Konsole. Als visuelle Hilfestellung f√ºr den Menschen.\n",
    "5. Close: Schlie√üt die Umgebung.\n",
    "\n",
    "## Setup\n",
    "Der Constructor nimmt als Parameter die aktuelle Konfiguration entgegen. Wichitige Variablen sind: \n",
    "* self.radar: Das Spielfeld, worauf der Agent seine Actionen ausf√ºhrt.\n",
    "* self.avialable_actions: Zu Beginn des Spiel stehen dort alle Koordinatenpaare des Spielfelds, da jedes Koordinatenpaar eine valides Ziel ist. Nach dem Beschuss eines Koordinatenpaares muss dieses Paar aus der Liste der validen Aktionen gestrichen werden. Wenn der Agent wiederholt auf ein Feld schie√üt, welches nicht mehr in den available_actions vorhanden ist, wird das Spiel sofort beendet, da dies ein fehlerhafter Zug war. \n",
    "* self.enemy_board, self.enemyShips: Das Gegnerische Board wird mit Nullen initialsiert. An den Stellen, auf denen ein Schiff liegt wird, eine 1 in das Array geschrieben. Die Schiffe werden durch die Methode place_ships zuf√§llig plaziert.\n",
    "* self.action_space: Ist abh√§ngig von der Spielfeldgr√∂√üe. Bei einem Spielfeld 5x5 besitzt der Actionspace eine Gr√∂√üe von 25, bei 10x10 betr√§gt die Gr√∂√üe 100 \n",
    "* self.observation_space: Dieser beinhaltet Werte von -1 bis 2, da es 4 Zust√§nde in dem Spiel gibt {'W': 0, 'X': 1, '#': 2, '0': -1}. Diese Werte existieren f√ºr jedes Feld.\n",
    "\n",
    "Die Methode set_up wird zu Beginn jedes Spiels aufgerufen. In dieser Methode werden das Radar und das gegnerische Spielfeld auf deren jeweiligen Startwerte gesetzt. Die avialable_actions werden wieder zur√ºckgesetzt, sodass jeder Zug valide ist, und die Schiffe werden auf dem gegnerischen Spielfeld plaziert. Somit hat man bei jedem Spiel die gleiche Ausgangslage.\n",
    "\n",
    "## Step\n",
    "In der Methode werden als erstes die Koordinaten aus dem ausgew√§hlten Action extrahiert. Anschlie√üend wird √ºberpr√ºft, ob die Action valide war. Wenn nicht, wird das Spiel abgebrochen und ein entsprechender negativer Reward vergeben. War die ausgew√§hlte Action valide, so wird das Feld, durch die shoot Methode, beschossen. Nach jedem Schuss wird √ºberpr√ºft, ob das Spiel vorbei ist und der entsprechende Reward vergeben. \n",
    "\n",
    "## Reset\n",
    "Diese Methode wird aufgerufen, wenn ein Spiel vorbei ist. Dabei spielt es keine Rolle, ob das Spiel durch einen invaliden Zug oder durch Gewinnen beendet wurde. In der reset Methode wird die set_up Methode aufgerufen, um ein neues Spiel spielen zu k√∂nnen\n",
    "\n",
    "## Reward\n",
    "Der Agent bekommt f√ºr jede Action einen Reward. Dabei unterscheidet sich der Reward bei den zwei Trainingvarianten.\n",
    "\n",
    "Bei dem Training auf valide Spielz√ºge bekommt der Agent nur einen Reward von +1 wenn er ein Feld beschie√üt, welches er vorher noch nicht beschossen hatte. Der Agent bekommt einen Reward von -1 wenn er ein Feld beschie√üt, welches er breits beschossen hat. \n",
    "\n",
    "Bei dem Training effektiv das Spiel zu gewinnen unterscheiden sich die Rewards: \n",
    "Valider Schuss: \n",
    "* Kein Treffer: 0 \n",
    "* Treffer: +20. Hierbei ist egal, ob der Treffer das Schiff versenkt\n",
    "* Gewonnen: Reward wird berechnet anhand der ben√∂tigten Z√ºge, d.h der Agent bekommt einen h√∂heren Reward, wenn er weniger Z√ºge ben√∂tigt: 100 * ((self.board_size*self.board_size) / self.steps) \n",
    "\n",
    "Invalider Schuss:\n",
    "Der Reward wird auf -1000 gesetzt. Diese Zahl ist so hoch gew√§hlt, damit man invalide Sch√ºsse gut erkennen kann, wenn man sich sp√§ter den Verlauf des Rewards anschaut.\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from gym.envs.registration import register\n",
    "from copy import deepcopy\n",
    "from random import randint, getrandbits\n",
    "from gym import spaces\n",
    "\n",
    "\"\"\"\n",
    "Class representing a ship object\n",
    "\"\"\"\n",
    "class Ship:\n",
    "    # A ship has zero hits initially\n",
    "    hits = 0\n",
    "\n",
    "    \"\"\"\n",
    "    Constructor for a Ship object\n",
    "    Arguments:\n",
    "    length = Size of the Ship.\n",
    "    x = Starting X coordinate\n",
    "    y = Starting Y coordinate\n",
    "    is_vertical = Boolean whether ship direction is vertically or horizontally\n",
    "    \"\"\"\n",
    "    def __init__(self, length, x, y, is_vertical):\n",
    "        self.length = length\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.is_vertical = is_vertical\n",
    "\n",
    "    '''\n",
    "    Method for counting hits.\n",
    "    '''\n",
    "    def hit(self):\n",
    "        self.hits += 1\n",
    "\n",
    "    '''\n",
    "    Method for checking if the ship is sunken.\n",
    "    '''\n",
    "    def sunken(self):\n",
    "        return self.hits == self.length\n",
    "\n",
    "    '''\n",
    "    Getter for length of a ship.\n",
    "    '''\n",
    "    def get_length(self):\n",
    "        return self.length\n",
    "\n",
    "    '''\n",
    "    Getter for starting x coordinate of a ship.\n",
    "    '''\n",
    "    def get_x(self):\n",
    "        return self.x\n",
    "\n",
    "    '''\n",
    "    Getter for starting y coordinate of a ship.\n",
    "    '''\n",
    "    def get_y(self):\n",
    "        return self.y\n",
    "\n",
    "    '''\n",
    "    Getter for is_vertical \n",
    "    '''\n",
    "    def get_is_vertical(self):\n",
    "        return self.is_vertical\n",
    "\n",
    "    '''\n",
    "    Method for checking shoot hitting a part of the ship.\n",
    "    x_hit: x Coordinate of the shoot\n",
    "    y_hit: y Coordinate of the shoot\n",
    "    '''\n",
    "    def is_hit(self, x_hit, y_hit):\n",
    "        # Iterate all parts of the ship\n",
    "        for i in range(self.length):\n",
    "            # Determine placement direction of the ship\n",
    "            if self.is_vertical:\n",
    "                # Check if ship part is hit\n",
    "                if self.x + i == x_hit and self.y == y_hit:\n",
    "                    # Update hit counter\n",
    "                    self.hit()\n",
    "                    return True\n",
    "            # Ship is placed horizontally\n",
    "            else:\n",
    "                # Check if ship part is hit\n",
    "                if self.x == x_hit and self.y + i == y_hit:\n",
    "                    # Update hit counter\n",
    "                    self.hit()\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Class representing the Battleship gym environment\n",
    "\"\"\"\n",
    "class BattleshipsEnv(gym.Env):\n",
    "  \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "  metadata = {'render.modes': ['human']}\n",
    "\n",
    "  \"\"\"\n",
    "  Constructor for the battleships OpenAi gym environment\n",
    "  Arguments:\n",
    "  config = Configuration Object for the battleships game.\n",
    "  \"\"\"\n",
    "  def __init__(self, config):\n",
    "    super(BattleshipsEnv, self).__init__()\n",
    "\n",
    "    # Instanciate variables\n",
    "    # Boolean if ships can touch each other\n",
    "    self.gap = config.gap\n",
    "    # Boolean if Agent is trained on making valid shoots or playing the game \n",
    "    self.binary_reward = config.binary_reward\n",
    "\n",
    "    # Boolean if ships are placed random each round or stay at fixed position.\n",
    "    self.static_placement = config.static_placement\n",
    "    self.placement = None\n",
    "    self.placement_ships = None\n",
    "\n",
    "    # The player board \"radar\" where he registers his shots.\n",
    "    self.radar = []\n",
    "\n",
    "    # List containing all valid actions (action get removed after beeing used/shot once)\n",
    "    self.available_actions = []\n",
    "\n",
    "    # List containing the ships objects of the enemy\n",
    "    self.enemyShips = []\n",
    "\n",
    "    # The enemy board where the ships of the enemy are placed\n",
    "    self.enemy_board = []\n",
    "\n",
    "    \"\"\"\n",
    "    FieldEncoding to map the state to more human readable content.\n",
    "    Water:0\n",
    "    Hit: *\n",
    "    Sunken: #\n",
    "    Miss: 0\n",
    "    \"\"\"\n",
    "    self.fieldEncoding = {'W': 0, 'X': 1, '#': 2, '0': -1}\n",
    "\n",
    "    # Set the available ships to place\n",
    "    self.ships = config.ships\n",
    "\n",
    "    # Set the size of the board\n",
    "    self.board_size = config.board_size\n",
    "\n",
    "    self.steps = 0\n",
    "\n",
    "    # Set up the game for a new round\n",
    "    self.set_up()\n",
    "\n",
    "    # Allocate possible actions (Each field can be shot => board_size*board_size)\n",
    "    self.action_space = spaces.Discrete(self.board_size * self.board_size)\n",
    "\n",
    "    # Allocate oberservations (Each field has a state and can be observed.\n",
    "    # Possible states are Integer values of fieldEncoding => low=-1, high=2)\n",
    "    self.observation_space = spaces.Box(low=-1, high=2, shape=(self.board_size, self.board_size),\n",
    "                                        dtype=np.int)\n",
    "\n",
    "  '''\n",
    "  Step function for OpenAi gym.\n",
    "  Gets called one per round in battleships.\n",
    "  Arguments:\n",
    "  action = Integer, the index of the next field to shoot.\n",
    "  '''\n",
    "  def step(self, action):\n",
    "    # Map the action to the board and get x,y coordinates of the next field to shot\n",
    "    x, y = np.unravel_index(action, (self.board_size, self.board_size))\n",
    "\n",
    "    # initialize reward with 0\n",
    "    reward = 0\n",
    "\n",
    "    #Check if x, y allready have been shot.\n",
    "    if (x, y) not in self.available_actions:\n",
    "\n",
    "      double_shot_reward = -1000\n",
    "\n",
    "      if self.binary_reward:\n",
    "        double_shot_reward = -1\n",
    "\n",
    "      return self.radar, double_shot_reward, True, {\n",
    "        'miss_count': 0,\n",
    "        'hit_count': 0,\n",
    "        'empty_count': 0,\n",
    "        'sunken_count': 0\n",
    "      }\n",
    "\n",
    "      # Add negative reward for shooting a forbidden field\n",
    "      #reward -= 2 * self.board_size\n",
    "\n",
    "      # Get a random index of available actions\n",
    "      random_index = np.random.randint(0, len(self.available_actions))\n",
    "\n",
    "      # Get x,y coordinates from a random valid available action\n",
    "      #x, y = self.available_actions[random_index]\n",
    "\n",
    "    # Shoot coordinates\n",
    "    hit = self.shoot(x, y)\n",
    "\n",
    "    self.steps += 1\n",
    "\n",
    "    # Evaluate result of shot\n",
    "    after_shot_state = self.radar\n",
    "    miss_after_shot, hit_after_shot, empty_after_shot, sunken_prev_shot = self.count_states(after_shot_state)\n",
    "\n",
    "    # Add count information to info, for debug and possible calculations of statistics\n",
    "    info = {\n",
    "      'miss_count': miss_after_shot,\n",
    "      'hit_count': hit_after_shot,\n",
    "      'empty_count': empty_after_shot,\n",
    "      'sunken_count': sunken_prev_shot\n",
    "    }\n",
    "\n",
    "    # Check if game is done\n",
    "    done = self.check_done()\n",
    "\n",
    "    # Calculate reward\n",
    "    reward += self.calculate_reward(hit, done, self.steps)\n",
    "\n",
    "    if self.binary_reward:\n",
    "      reward = 1\n",
    "\n",
    "    return after_shot_state, reward, done, info\n",
    "\n",
    "  # OpenAI gym reset method. Gets called to set up a new game.\n",
    "  def reset(self):\n",
    "    self.set_up()\n",
    "\n",
    "    # Return the fresh board of the player\n",
    "    return self.radar\n",
    "\n",
    "  # OpenAi gym render method\n",
    "  def render(self, mode='human'):\n",
    "    for x in range(self.board_size):\n",
    "      # Print horizontale line\n",
    "      print(\"-\" * (4 * self.board_size + 2))\n",
    "      for y in range(self.board_size):\n",
    "        # Get state of the current field\n",
    "        current_state_value = self.radar[x, y]\n",
    "        # Get fieldEncoding for the state of the current field\n",
    "        current_state = list(self.fieldEncoding.keys())[list(self.fieldEncoding.values()).index(current_state_value)]\n",
    "        # If state of current Field is Water, print empty space for better readability\n",
    "        current_state = (current_state if current_state != \"W\" else \" \")\n",
    "        # Print vertical lines\n",
    "        print(\" | \", end=\"\")\n",
    "        # Print current state\n",
    "        print(current_state, end=\"\")\n",
    "      print(\" |\")\n",
    "    print(\"-\" * (4 * self.board_size + 2))\n",
    "\n",
    "  # OpenAi gym close method\n",
    "  def close (self):\n",
    "    print('close')\n",
    "\n",
    "  # Method to place ships on a given board\n",
    "  def place_ships(self, board):\n",
    "    # Initialize variables\n",
    "    x = 0\n",
    "    y = 0\n",
    "    ships = []\n",
    "    all_ships_placed = False\n",
    "\n",
    "    # Try to place ships until all ships could be placed\n",
    "    while not all_ships_placed:\n",
    "      # Reset placed ships list\n",
    "      ships = []\n",
    "      # Reset/Clean the board\n",
    "      for x in range(self.board_size):\n",
    "        for y in range(self.board_size):\n",
    "          board[x, y] = 0\n",
    "      # No ships have been placed\n",
    "      ships_placed = 0\n",
    "      reset = False\n",
    "\n",
    "      # Iterate over all ships to place\n",
    "      for ship_length in self.ships:\n",
    "        # Negative assumption, that the space is occupied\n",
    "        occupied = True\n",
    "        trys = 0\n",
    "\n",
    "        # Try to find a spot to place the ship\n",
    "        while occupied and not reset:\n",
    "          # Get new random coordinates on the board\n",
    "          x, y = self.get_random_coordinates()\n",
    "          # Get a new random alignment of the ship\n",
    "          is_vertical = bool(getrandbits(1))\n",
    "          #Check if the space is allready occupied\n",
    "          occupied = self.check_occupation(x, y, board, ship_length, is_vertical)\n",
    "          # Increment the trys for the current ship\n",
    "          trys += 1\n",
    "          # If no valid spot can be found, the placement of all ships must be reset\n",
    "          if trys > 1000:\n",
    "            reset = True\n",
    "        # If a valid spot is found:\n",
    "        if not reset:\n",
    "          # Create the ship and append it to the ships list\n",
    "          ships.append(Ship(ship_length, x, y, is_vertical))\n",
    "          # Increment the number of ships placed\n",
    "          ships_placed += 1\n",
    "\n",
    "          # Draw the placed ship onto the board\n",
    "          if is_vertical:\n",
    "            for i in range(ship_length):\n",
    "              board[x + i, y] = 1\n",
    "          else:\n",
    "            for i in range(ship_length):\n",
    "              board[x, y + i] = 1\n",
    "      # Check if all ships where placed\n",
    "      if ships_placed == len(self.ships):\n",
    "        all_ships_placed = True\n",
    "\n",
    "      if self.static_placement and self.placement is None:\n",
    "        self.placement = np.copy(board)\n",
    "        self.placement_ships = deepcopy(ships)\n",
    "\n",
    "    return ships\n",
    "\n",
    "  '''\n",
    "  Method check if the space for a possible ship is occupied on a given boad.\n",
    "  Arguments:\n",
    "  x = X Coordinate to place the first part of the ship\n",
    "  y = Y Coordinate to place the first part of the ship\n",
    "  board = Board where to check if the ship can be placed on\n",
    "  ship_length = Length of ship to place\n",
    "  is_vertical = Boolean if the ship should be placed vertically or horizontally\n",
    "  '''\n",
    "  def check_occupation(self, x, y, board, ship_length, is_vertical):\n",
    "    # Check if ships can touch.\n",
    "    if self.gap:\n",
    "      # Check for occupation with a gap\n",
    "      occupied = self.check_occupation_with_gap(x, y, board, ship_length, is_vertical)\n",
    "    else:\n",
    "      # Positive assumption that the space is not occupied\n",
    "      occupied = False\n",
    "\n",
    "      # Determine which direction to check\n",
    "      if is_vertical:\n",
    "        # Iterate all field of a ship, beginning from the start position (x,y)\n",
    "        # if the are allready occupied and inside the boundaries of the board\n",
    "        for i in range(ship_length):\n",
    "          if x + i >= self.board_size - 1:\n",
    "            return True\n",
    "          if board[x + i, y] == 1:\n",
    "            occupied = True\n",
    "      else:\n",
    "        # Same as previous, but horizontally\n",
    "        for i in range(ship_length):\n",
    "          if y + i >= self.board_size - 1:\n",
    "            return True\n",
    "          if board[x, y + i] == 1:\n",
    "            occupied = True\n",
    "\n",
    "    return occupied\n",
    "\n",
    "  '''\n",
    "  Method for checking occupation with a gap.\n",
    "  If a ship is placed vertical, it checks the top row for given coordinate.\n",
    "  Arguments:\n",
    "  x = X Coordinate to check on the board\n",
    "  y = y Coordinate to check on the board\n",
    "  board = Board to check for occupation\n",
    "  '''\n",
    "  def check_occupation_with_gap_top_row_is_vertical(self, x, y, board):\n",
    "    # If x Coordinate is top row of board, no further checks are needed\n",
    "    if x == 0:\n",
    "      return True\n",
    "    # Check y Coordinate is in bound and left upper field is occupied\n",
    "    if y != 0 and board[x - 1, y - 1] == 1:\n",
    "      return True\n",
    "    # Check center upper field is occupied\n",
    "    if board[x - 1, y] == 1:\n",
    "      return True\n",
    "    # Check y Coordinate is in bound and right upper field is occupied\n",
    "    if y != self.board_size - 1 and board[x - 1, y + 1] == 1:\n",
    "      return True\n",
    "\n",
    "  '''\n",
    "  Method for checking occupation with a gap.\n",
    "  If a ship is placed vertical, it checks the left and right column of given coordinate.\n",
    "  Arguments:\n",
    "  x = X Coordinate to check on the board\n",
    "  y = y Coordinate to check on the board\n",
    "  board = Board to check for occupation\n",
    "  '''\n",
    "  def check_occupation_with_gap_left_right_is_vertical(self, x, y, board):\n",
    "    # Check y Coordinate is in bound and left field is occupied\n",
    "    if y != 0 and board[x, y - 1] == 1:\n",
    "      return True\n",
    "    # Check y Coordinate is in bound and right field is occupied\n",
    "    if y != self.board_size - 1 and board[x, y + 1] == 1:\n",
    "      return True\n",
    "\n",
    "  '''\n",
    "  Method for checking occupation with a gap.\n",
    "  If a ship is placed vertical, it checks the bottom row of given coordinate.\n",
    "  Arguments:\n",
    "  x = X Coordinate to check on the board\n",
    "  y = y Coordinate to check on the board\n",
    "  board = Board to check for occupation\n",
    "  '''\n",
    "  def check_occupation_with_gap_bottom_row_is_vertical(self, x, y, board):\n",
    "    # If x Coordinate is bottom row of board, no further checks are needed\n",
    "    if x == self.board_size - 1:\n",
    "      return True\n",
    "    # Check y Coordinate is in bound and left lower field is occupied\n",
    "    if y != 0 and board[x + 1, y - 1] == 1:\n",
    "      return True\n",
    "    # Check center lower field is occupied\n",
    "    if board[x + 1, y] == 1:\n",
    "      return True\n",
    "    # Check y Coordinate is in bound and right lower field is occupied\n",
    "    if y != self.board_size - 1 and board[x + 1, y + 1] == 1:\n",
    "      return True\n",
    "\n",
    "  '''\n",
    "  Method for checking occupation with a gap.\n",
    "  If a ship is placed horizontally, it checks the left column of given coordinate.\n",
    "  Arguments:\n",
    "  x = X Coordinate to check on the board\n",
    "  y = y Coordinate to check on the board\n",
    "  board = Board to check for occupation\n",
    "  '''\n",
    "  def check_occupation_with_gap_left_column(self, x, y, board):\n",
    "    # If y Coordinate is left column of board, no further checks are needed\n",
    "    if y == 0:\n",
    "      return True\n",
    "    # Check x Coordinate is in bound and left upper field is occupied\n",
    "    if x != 0 and board[x - 1, y - 1] == 1:\n",
    "      return True\n",
    "    # Check center left field is occupied\n",
    "    if board[x, y - 1] == 1:\n",
    "      return True\n",
    "    # Check x Coordinate is in bound and left lower field is occupied\n",
    "    if x != self.board_size - 1 and board[x + 1, y - 1] == 1:\n",
    "      return True\n",
    "\n",
    "  '''\n",
    "  Method for checking occupation with a gap.\n",
    "  If a ship is placed horizontally, it checks the top and bottom row of given coordinate.\n",
    "  Arguments:\n",
    "  x = X Coordinate to check on the board\n",
    "  y = y Coordinate to check on the board\n",
    "  board = Board to check for occupation\n",
    "  '''\n",
    "  def check_occupation_with_gap_top_bottom(self, x, y, board):\n",
    "    # Check x Coordinate is in bound and top field is occupied\n",
    "    if x != 0 and board[x - 1, y] == 1:\n",
    "      return True\n",
    "    # Check x Coordinate is in bound and bottom field is occupied\n",
    "    if x != self.board_size - 1 and board[x + 1, y] == 1:\n",
    "      return True\n",
    "\n",
    "  '''\n",
    "  Method for checking occupation with a gap.\n",
    "  If a ship is placed horizontally, it checks the right column of given coordinate.\n",
    "  Arguments:\n",
    "  x = X Coordinate to check on the board\n",
    "  y = y Coordinate to check on the board\n",
    "  board = Board to check for occupation\n",
    "  '''\n",
    "  def check_occupation_with_gap_right_column(self, x, y, board):\n",
    "    # If y Coordinate is right column of board, no further checks are needed\n",
    "    if y == self.board_size - 1:\n",
    "      return True\n",
    "    # Check x Coordinate is in bound and right upper field is occupied\n",
    "    if x != 0 and board[x - 1, y + 1] == 1:\n",
    "      return True\n",
    "    # Check center right field is occupied\n",
    "    if board[x, y + 1] == 1:\n",
    "      return True\n",
    "    # Check x Coordinate is in bound and right lower field is occupied\n",
    "    if x != self.board_size - 1 and board[x + 1, y + 1] == 1:\n",
    "      return True\n",
    "\n",
    "  '''\n",
    "  Method for checking occupation with a gap.\n",
    "  Check whether ship is placed vertically or horizontally. \n",
    "  Makes additional checks if coordinates are occupied.\n",
    "  x = X Coordinate start coordinate to place the ship\n",
    "  y = y Coordinate start coordinate to place the ship\n",
    "  board = Board to place the ship on \n",
    "  ship_length = Length of the ship to place\n",
    "  is_vertical = Boolean if the ship should be placed vertically or horizontally\n",
    "  '''\n",
    "  def check_occupation_with_gap(self, x, y, board, ship_length, is_vertical):\n",
    "    # Possitive assumption the field is never occupied\n",
    "    occupied = False\n",
    "    # Determine ship placement direction\n",
    "    if is_vertical:\n",
    "      # Iterate all field of a ship, beginning from the start position (x,y)\n",
    "      for i in range(ship_length):\n",
    "        current_field_x = x + i\n",
    "        # Check field is inside the boundaries of the board\n",
    "        if current_field_x >= self.board_size or board[current_field_x, y] == 1:\n",
    "          occupied = True\n",
    "          break\n",
    "        # Check first part of the ship\n",
    "        if i == 0:\n",
    "          # Check top row\n",
    "          if self.check_occupation_with_gap_top_row_is_vertical(current_field_x, y, board):\n",
    "            occupied = True\n",
    "            break\n",
    "          # Check left and right column\n",
    "          if self.check_occupation_with_gap_left_right_is_vertical(current_field_x, y, board):\n",
    "            occupied = True\n",
    "            break\n",
    "        # Check middle parts of the ship\n",
    "        elif i < ship_length - 1:\n",
    "          # Check left and right column\n",
    "          if self.check_occupation_with_gap_left_right_is_vertical(current_field_x, y, board):\n",
    "            occupied = True\n",
    "            break\n",
    "        # Check last part of the ship\n",
    "        else:\n",
    "          # Check bottom row\n",
    "          if self.check_occupation_with_gap_bottom_row_is_vertical(current_field_x, y, board):\n",
    "            occupied = True\n",
    "            break\n",
    "          # Check left and right column\n",
    "          if self.check_occupation_with_gap_left_right_is_vertical(current_field_x, y, board):\n",
    "            occupied = True\n",
    "            break\n",
    "    # Ship placement direction is horizontal\n",
    "    else:\n",
    "      # Iterate all field of a ship, beginning from the start position (x,y)\n",
    "      for i in range(ship_length):\n",
    "        current_field_y = y + i\n",
    "        # Check field is inside the boundaries of the board\n",
    "        if current_field_y >= self.board_size or board[x, current_field_y] == 1:\n",
    "          occupied = True\n",
    "          break\n",
    "        # Check first part of the ship\n",
    "        if i == 0:\n",
    "          # Check left column\n",
    "          if self.check_occupation_with_gap_left_column(x, current_field_y, board):\n",
    "            occupied = True\n",
    "            break\n",
    "          # Check top and bottom row\n",
    "          if self.check_occupation_with_gap_top_bottom(x, current_field_y, board):\n",
    "            occupied = True\n",
    "            break\n",
    "        # Check middle parts of the ship\n",
    "        elif i < ship_length - 1:\n",
    "          # Check top and bottom row\n",
    "          if self.check_occupation_with_gap_top_bottom(x, current_field_y, board):\n",
    "            occupied = True\n",
    "            break\n",
    "        # Check last part of ship\n",
    "        else:\n",
    "          # Check right column\n",
    "          if self.check_occupation_with_gap_right_column(x, current_field_y, board):\n",
    "            occupied = True\n",
    "            break\n",
    "          # Check top and bottom row\n",
    "          if self.check_occupation_with_gap_top_bottom(x, current_field_y, board):\n",
    "            occupied = True\n",
    "            break\n",
    "\n",
    "    return occupied\n",
    "\n",
    "  '''\n",
    "  Method for creating a pair of random coordinates.\n",
    "  return: Pair of random coordinates\n",
    "  '''\n",
    "  def get_random_coordinates(self):\n",
    "      # random number in range 0 - board_size - 1\n",
    "      x = randint(0, self.board_size - 1)\n",
    "      y = randint(0, self.board_size - 1)\n",
    "      return x, y\n",
    "\n",
    "  '''\n",
    "  Method for counting all states currently present on the radar board\n",
    "  state: Numpy Array of all states present on the radar board\n",
    "  '''\n",
    "  def count_states(self, state):\n",
    "    # Counts how many times a unique element is present on the radar board\n",
    "    # return the unique element and how many times the unique element is present\n",
    "    uni_states, counts = np.unique(state.ravel(), return_counts=True)\n",
    "    hit = counts[uni_states == self.fieldEncoding['X']]\n",
    "    miss = counts[uni_states == self.fieldEncoding['0']]\n",
    "    empty = counts[uni_states == self.fieldEncoding['W']]\n",
    "    sunken = counts[uni_states == self.fieldEncoding['#']]\n",
    "    # Checks whether hits present and assigns present hits\n",
    "    if len(hit) == 0:\n",
    "      hit = 0\n",
    "    else:\n",
    "      hit = hit[0]\n",
    "    # Checks whether misses present and assigns present misses\n",
    "    if len(miss) == 0:\n",
    "      miss = 0\n",
    "    else:\n",
    "      miss = miss[0]\n",
    "    # Checks whether empty fields present and assigns present empty fields\n",
    "    if len(empty) == 0:\n",
    "      empty = 0\n",
    "    else:\n",
    "      empty = empty[0]\n",
    "    # Checks whether sunken ships present and assigns present sunken ships\n",
    "    if len(sunken) == 0:\n",
    "      sunken = 0\n",
    "    else:\n",
    "      sunken = sunken[0]\n",
    "\n",
    "    return miss, hit, empty, sunken\n",
    "\n",
    "  '''\n",
    "  Method for shooting on the enemy board.\n",
    "  Displays the result on radar board\n",
    "  x: X Coordinate to shoot\n",
    "  y: Y Coordinate to shoot\n",
    "  '''\n",
    "  def shoot(self, x, y):\n",
    "    # Negative assumption shoot misses a ship\n",
    "    hit = False\n",
    "    # Radar board field is set to miss\n",
    "    self.radar[x, y] = self.fieldEncoding['0']\n",
    "    # Iterate enemyShips to check for hits\n",
    "    for ship in self.enemyShips:\n",
    "      # Check whether shoot is a hit\n",
    "      if ship.is_hit(x, y):\n",
    "        # Set radar board field to hit\n",
    "        self.radar[x, y] = self.fieldEncoding['X']\n",
    "        hit = True\n",
    "        # Check whether the ship is sunken\n",
    "        if ship.sunken():\n",
    "          # Set radar board ship fields to sunken\n",
    "          self.draw_sunken(ship, self.radar)\n",
    "\n",
    "    # Remove shoot Coordinate from list of available actions\n",
    "    self.available_actions.remove((x, y))\n",
    "    return hit\n",
    "\n",
    "  '''\n",
    "  Method for displaying a sunken ship on radar board.\n",
    "  ship: Sunken ship\n",
    "  board: Radar Board \n",
    "  '''\n",
    "  def draw_sunken(self, ship, board):\n",
    "    # Get start coordinates of the sunken ship\n",
    "    x = ship.get_x()\n",
    "    y = ship.get_y()\n",
    "    # Determine direction of the sunken ship\n",
    "    if ship.is_vertical:\n",
    "      # Iterate the sunken ship and update radar board fields to sunken\n",
    "      for i in range(ship.get_length()):\n",
    "        board[x + i, y] = self.fieldEncoding['#']\n",
    "    # sunken ship is placed horizontally\n",
    "    else:\n",
    "      # Iterate the sunken ship and update radar board fields to sunken\n",
    "      for i in range(ship.get_length()):\n",
    "        board[x, y + i] = self.fieldEncoding['#']\n",
    "\n",
    "  '''\n",
    "  Method for checking if the Game is Done.\n",
    "  All enemy ships must be sunken. \n",
    "  '''\n",
    "  def check_done(self):\n",
    "    # Possitive assumption the game is allways done\n",
    "    if self.binary_reward:\n",
    "      done = False\n",
    "    else:\n",
    "      done = True\n",
    "    #done = False\n",
    "    # Iterate all enemy ships\n",
    "    # Check if one of the enemy ships is not yet sunken\n",
    "    if self.binary_reward:\n",
    "      if not self.available_actions:\n",
    "        done = True\n",
    "    else:\n",
    "      for ship in self.enemyShips:\n",
    "        if not ship.sunken():\n",
    "          done = False\n",
    "    return done\n",
    "\n",
    "  '''\n",
    "  Method for calculating the reward.\n",
    "  This method must be updated for reward based learning. Currently uses dummy values\n",
    "  reward: Current Reward of the Agent\n",
    "  hit: Boolean last shoot was hit or miss \n",
    "  done: Boolean game is finished\n",
    "  '''\n",
    "  def calculate_reward(self, hit, done, steps):\n",
    "    # Agent gets a reward for hitting a ship\n",
    "    reward = 0\n",
    "    if hit:\n",
    "      reward += 20\n",
    "    # Agent gets more reward if he finishes the game\n",
    "    if done:\n",
    "     reward += 100 * ((self.board_size*self.board_size) / self.steps)\n",
    "    # Float for later calculations\n",
    "    return int(round(reward))\n",
    "\n",
    "  '''\n",
    "  Method to setup the game environment.\n",
    "  '''\n",
    "  def set_up(self):\n",
    "    # Inits radar board with Water fields\n",
    "    self.radar = self.fieldEncoding['W'] * np.ones((self.board_size, self.board_size), dtype='int')\n",
    "    # Inits enemy board with zeros representing water\n",
    "    self.enemy_board = 0 * np.ones((self.board_size, self.board_size), dtype='int')\n",
    "    # resets available_actions\n",
    "    self.available_actions = []\n",
    "    # Init available_actions for all fields of the board\n",
    "    for x in range(self.board_size):\n",
    "      for y in range(self.board_size):\n",
    "        self.available_actions.append((x, y))\n",
    "    # places enemy ships on the enemy board\n",
    "    if self.placement_ships and self.placement is not None:\n",
    "      self.enemyShips = deepcopy(self.placement_ships)\n",
    "      self.enemy_board = np.copy(self.placement)\n",
    "    else:\n",
    "      self.enemyShips = self.place_ships(self.enemy_board)\n",
    "\n",
    "    self.steps = 0\n",
    "\n",
    "  '''\n",
    "  Method calculates the maximum mean reward threshold for the callback in training. \n",
    "  '''\n",
    "  def calculate_threshold(self):\n",
    "    if self.binary_reward:\n",
    "      return self.board_size * self.board_size\n",
    "    else:\n",
    "      ship_fields = 0\n",
    "      reward = 0\n",
    "      for ship_length in self.ships:\n",
    "        ship_fields += ship_length\n",
    "      reward += ship_fields * 20\n",
    "      reward += 100 * ((self.board_size*self.board_size) / ship_fields)\n",
    "      return int(round(reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class representing a config object\n",
    "\"\"\"\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Constructor for a Config object\n",
    "    Arguments:\n",
    "    board_size = Number of fields in x and y direction.\n",
    "    ships = Array of Ships with length of ships. Example [2,2,3] Sets 2 Ships with length of 2, 1 ship with length 3\n",
    "    gap = Boolean: True => Ships have a gap of at least 1 water field between each other.\n",
    "    False => Ships can be placed side by side\n",
    "    static_placement => A static placement for ships over all iterations is used\n",
    "    binary_reward => The reward will be +1 for a valid action and -1 for an invalid action\n",
    "    to train an agent not to choose the same action multiple times.\n",
    "    \"\"\"\n",
    "    def __init__(self, board_size, ships, gap, static_placement, binary_reward):\n",
    "        self.board_size = board_size\n",
    "        self.ships = ships\n",
    "        self.gap = gap\n",
    "        self.static_placement = static_placement\n",
    "        self.binary_reward = binary_reward\n",
    "        \n",
    "\"\"\"\n",
    "Class representing a result object\n",
    "\"\"\"\n",
    "class Result:\n",
    "    \"\"\"\n",
    "    Constructor for a result object\n",
    "    Arguments:\n",
    "    history = List of Tuples with (rounds, action, observation, reward, done, info).\n",
    "    reward = Overall reward of a played game\n",
    "    rounds = Amount of rounds used to finish the game\n",
    "    \"\"\"\n",
    "    def __init__(self, history=None, reward=0, rounds=0):\n",
    "        if history is None:\n",
    "            history = []\n",
    "        self.history = history\n",
    "        self.overall_reward = reward\n",
    "        self.rounds = rounds\n",
    "        self.hit_miss_ratio = 0\n",
    "\n",
    "    '''\n",
    "    Method for appending a round history to result.\n",
    "    round: Number of the current round\n",
    "    action: Performed action\n",
    "    observation: Radar Board\n",
    "    reward: Current reward for the performed action\n",
    "    done: Boolean if game has finished\n",
    "    info: Debug info\n",
    "    '''\n",
    "    def append_history(self, round, action, observation, reward, done, info):\n",
    "        self.history.append((round, action, observation, reward, done, info))\n",
    "        self.calculate_overall_reward(reward)\n",
    "\n",
    "    '''\n",
    "    Method for setting a the number of rounds to result.\n",
    "    rounds: Number of rounds used to finish the game\n",
    "    '''\n",
    "    def set_rounds(self, rounds):\n",
    "        self.rounds = rounds\n",
    "\n",
    "    '''\n",
    "    Method for calculating the hit/miss ratio.\n",
    "    hit: Amount of hits\n",
    "    miss: Amount of misses\n",
    "    '''\n",
    "    def calculate_hit_miss_ratio(self, hit, miss):\n",
    "        if miss is not 0:\n",
    "            self.hit_miss_ratio = hit / miss\n",
    "\n",
    "    '''\n",
    "    Method for calculating the overall reward.\n",
    "    reward: current reward of the reward\n",
    "    '''\n",
    "    def calculate_overall_reward(self, reward):\n",
    "        self.overall_reward += reward\n",
    "\n",
    "    '''\n",
    "    Getter for the overall_reward of result object.\n",
    "    '''\n",
    "    def get_overall_reward(self):\n",
    "        return self.overall_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welche Probleme entstehen durch einen festen Action-Space?\n",
    "\n",
    "Die Probleme, die bei einem festen Action-Space entstehen k√∂nnen sind im Beispiel von Schiffe versenken, dass der Agent auf bereits beschossene Felder schie√üen kann. Die Felder, auf die bereits geschossen worden ist, werden nicht aus dem Action-Space entfernt und der Agent kann so mehrfach auf das gleiche Feld schie√üen. Dies f√ºhrt zu verf√§lschten Ergebnissen und verhindert ein sinvolles Training eines Agenten. Der Action-Space kann in einer OpenAI Gym Umgebung, nicht zur Laufzeit ver√§ndert bzw. angepasst werden. Der Agent muss somit selber √ºber das Wissen verf√ºgen, ob eine Aktion valide ist oder nicht. Dieses Verhalten kann dem Agenten antrainiert werden, dies wird im folgedem Abschnitt erl√§utert. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wie kann ein Agent \"valide Z√ºge\" erkennen und lernen?\n",
    "Damit das Problem des festen Action-Spaces umgangen werden kann, muss der Agent lernen,   \n",
    "nur valide Spielz√ºge auszuf√ºhren. Ein Agent lernt jedoch √ºber eine Maximierung des Rewards.  \n",
    "Dies f√ºhrt zu einem Problem, da z. B. ein positiver Reward f√ºr das Treffen eines Schiffes vergeben wird.  \n",
    "Dies Konssequenz ist, dass genau das Gegenteil zum gew√ºnschten Verhalten eintritt. Trifft ein Agent  \n",
    "ein Schiff, so erh√§lt dieser einen positiven Reward und f√ºhrt diesen Zug erneut aus, um den positiven Reward  \n",
    "erneut zu erhalten. Damit dieses Verhalten nicht auftritt, muss ein Agent in 2 Schritten trainiert werden.  \n",
    "Im ersten Schritt muss der Agent lernen, kein Feld doppelt zu beschie√üen.\n",
    "Sobald der Agent dieses Verhalten ausreichend erlernt hat, kann mit diesem \"SingleShot\"-Modell\n",
    "das schnellstm√∂gliche Versenken der Schiffe trainiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wie wurde dies im Projekt umgesetzt?\n",
    "Der Agent muss lernen, zwischen validen und nicht validen Spielz√ºgen zu unterscheiden.\n",
    "Damit der Agent lernt, nur valide Spielz√ºge auszuf√ºhren, werden folgende Rewards verteilt:  \n",
    "* -1 F√ºr den Beschuss eines bereits beschossenen Feldes und Abbruch des Spiels.\n",
    "    ```\n",
    "      # BattleshipsEnv.py Z:91\n",
    "      if self.binary_reward:\n",
    "        double_shot_reward = -1\n",
    "\n",
    "      return self.radar, double_shot_reward, True, {\n",
    "        'miss_count': 0,\n",
    "        'hit_count': 0,\n",
    "        'empty_count': 0,\n",
    "        'sunken_count': 0\n",
    "      }\n",
    "    ```\n",
    "* +1 F√ºr den Beschuss eines neuen Feldes.\n",
    "    ```\n",
    "    # BattleshipsEnv.py Z:133\n",
    "    if self.binary_reward:\n",
    "      reward = 1\n",
    "    ``` \n",
    "Treffer von Schiffen werden hierbei au√üer Acht gelassen, sodass der h√∂chste Reward erreicht wird, falls alle Felder einmal beschossen werden. Aufgrund des -1 bzw. +1 Reward, wird diese Rewardverteilung im weitern Verlauf als \"Binary\" bzw. \"Binary Reward\" bezeichnet. Dieser Binary-Modus kann mithilfe eines Boolean in der Konfiguration festgelegt werden:\n",
    "    ```\n",
    "    # TrainACKTR.py Z:12\n",
    "    # Erstellen der Config und der Environment\n",
    "    # Der letzte Parameter gibt den Binary-Modus an: True = Binary\n",
    "    config = Config(5, [3, 2, 2], True, False, True)\n",
    "    env2 = gym.make('Battleships-v0', config=config)\n",
    "    ```\n",
    "Folgend kann der Agent z. B. ACKTR so auf valide Spielz√ºge trainiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_battleships\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.common.callbacks import CheckpointCallback, EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines import ACKTR\n",
    "\n",
    "# Inits Battleship gym environments and config\n",
    "config = Config(5, [3, 2, 2], True, False, True)\n",
    "env2 = gym.make('Battleships-v0', config=config)\n",
    "env3 = gym.make('Battleships-v0', config=config)\n",
    "env = DummyVecEnv([lambda: env2])\n",
    "env4 = DummyVecEnv([lambda: env3])\n",
    "\n",
    "\n",
    "# Define Callback\n",
    "#Callback stops training if maximum is reached in mean reward\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=env2.calculate_threshold(), verbose=1)\n",
    "# Callback safes the currently best model\n",
    "eval_callback = EvalCallback(env4, callback_on_new_best=callback_on_best, verbose=1, best_model_save_path='./ACKTR_Models/best/')\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1e4, save_path='./model_checkpoints/')\n",
    "\n",
    "\n",
    "# Uncomment, to train a new fresh model, otherwise a allready trained model will be trained\n",
    "model = ACKTR(MlpPolicy, env, verbose=2, tensorboard_log=\"./logs/progress_tensorboard/\",  n_cpu_tf_sess=4)\n",
    "\n",
    "# Train model\n",
    "# Only 100.000 for demonstration purposes, otherwise use at least 1.000.000\n",
    "model.learn(100000, callback=[checkpoint_callback, eval_callback])\n",
    "\n",
    "# Delete current model and load the best model\n",
    "del model\n",
    "model = ACKTR.load(\"./ACKTR_Models/best/best_model.zip\", verbose=2, env=env, tensorboard_log=\"./logs/progress_tensorboard/\")\n",
    "\n",
    "\n",
    "\n",
    "# Test trained model\n",
    "results = []\n",
    "for iteration in range(100):\n",
    "    score = 0\n",
    "    print('Iteration', iteration)\n",
    "    # Observed Player board\n",
    "    observation = env.reset()\n",
    "    # Init new Result\n",
    "    result = Result()\n",
    "    done = False\n",
    "    # Amount of moves used to finish the game\n",
    "    rounds = 0\n",
    "    while not done:\n",
    "        rounds += 1\n",
    "        # Get a random action from the action space\n",
    "        # action = env.action_space.sample()\n",
    "        # Agent performs a step\n",
    "        # observation, reward, done, info = env.step(action)\n",
    "        action, _states = model.predict(observation)\n",
    "        nextObservation, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        # Add step to result Object\n",
    "        result.append_history(rounds, action, nextObservation, reward, done, info)\n",
    "        observation = nextObservation\n",
    "        # Game is done\n",
    "        if done:\n",
    "            print(\"End of game: overall_reward=\", result.get_overall_reward(), \",rounds\", rounds, \"score\", score)\n",
    "            # Store amount of rounds in result object\n",
    "            result.set_rounds(rounds)\n",
    "            # Add current result object to all results\n",
    "            results.append(result)\n",
    "print('Finished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mithilfe des [Tensorboards](https://www.tensorflow.org/tensorboard) kann der Fortschritt der Agenten w√§hrend des Trainings  \n",
    "beobachtet werden. Um das Tensorboard zu starten, muss folgender Befehl, in der Konsole ausgef√ºhrt werden:  \n",
    "```\n",
    "tensorboard --port 6004 --logdir ./logs/progress_tensorboard/\n",
    "```\n",
    "Die Ausgabe im Tensorboard sieht wie folgt aus:  \n",
    "![tensorboard](./images/tensorboard.png)  \n",
    "Hierbei ist zu erkennen, dass sich der \"Episode_Reward\" dem Wert 25 und somit dem Optimum eines 5x5 Feldes ann√§hert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da der Agent nun ausreichend valide Spielz√ºge beherrscht, kann dieser nun darauf trainiert werden,\n",
    "mit m√∂glichst wenig Spielz√ºgen bzw. Runden, alle Schiffe zu versenken. Ist der Binary-Modus nicht ausgew√§hlt(False),  \n",
    "so werden die Rewards auf eine andere Art und Weise verteilt bzw. berechnet:\n",
    "* -1000 falls ein Feld doppelt beschossen wird, das Spiel wird im Anschluss abgebrochen\n",
    "    ```\n",
    "      # BattleshipsEnv.py Z: 89\n",
    "      # Festlegen des negativ Rewards f√ºr doppelten Beschuss\n",
    "      double_shot_reward = -1000\n",
    "\n",
    "      # Im Binary Mode wird -1000 mit -1 √ºberschrieben\n",
    "      if self.binary_reward:\n",
    "        double_shot_reward = -1\n",
    "       \n",
    "      # Reward zur√ºckgeben und Spiel abbrechen\n",
    "      return self.radar, double_shot_reward, True, {\n",
    "        'miss_count': 0,\n",
    "        'hit_count': 0,\n",
    "        'empty_count': 0,\n",
    "        'sunken_count': 0\n",
    "      }\n",
    "    ```\n",
    "* Berechnung des Rewards auf der Basis ob ein Schiff getroffen wurde oder nicht\n",
    "    ```\n",
    "    # BattleshipsEnv.py Z: 110\n",
    "    # Shoot coordinates\n",
    "    hit = self.shoot(x, y)\n",
    "    \n",
    "    [...]\n",
    "    \n",
    "    # BattleshipsEnv.py Z: 130\n",
    "    # Reward berechnen\n",
    "    reward += self.calculate_reward(hit, done, self.steps)\n",
    "    \n",
    "    # Falls Binary-Mode wird der Reward mit +1 √ºberschrieben\n",
    "    if self.binary_reward:\n",
    "      reward = 1\n",
    "    \n",
    "    # Reward, state, info, done zur√ºckgeben\n",
    "    return after_shot_state, reward, done, info\n",
    "    ```\n",
    "    Der Reward wird wie folgt berechnet:\n",
    "    ```\n",
    "    '''\n",
    "    Method for calculating the reward.\n",
    "    This method must be updated for reward based learning.\n",
    "    hit: Boolean last shoot was hit or miss \n",
    "    done: Boolean game is finished\n",
    "    '''\n",
    "    def calculate_reward(self, hit, done):\n",
    "      # Agent gets a reward for hitting a ship\n",
    "      reward = 0\n",
    "      if hit:\n",
    "        reward += 20\n",
    "      # Agent gets more reward if he finishes the game\n",
    "      if done:\n",
    "       reward += 100 * ((self.board_size*self.board_size) / self.steps)\n",
    "      return int(round(reward))\n",
    "    ```\n",
    "    * +20 Reward f√ºr den Treffer eines Schiffs\n",
    "    * +100 x (25 / Ben√∂tigte Rundenanzahl) Bei einem 5x5 Spielfeld, die minimale Rundenanzahl   \n",
    "    bei einer Belegung mit 3,2,2, ist 7. Dies w√ºrde somit einem maximalen Bonus-Reward von   \n",
    "    100 X (25/7) = 157(gerundet) entsprechen.    \n",
    "        \n",
    "Da somit positive Rewards f√ºr Treffer von Schiffen vergeben werden, lernt der Agent m√∂glichst schnell, alle Schiffe zu versenken. Das zuvor trainierte Modell kann nun wie folgt weiter trainiert werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.common.callbacks import CheckpointCallback, EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines import ACKTR\n",
    "\n",
    "# Inits Battleship gym environments and config\n",
    "config = Config(5, [3, 2, 2], True, False, False)\n",
    "env2 = gym.make('Battleships-v0', config=config)\n",
    "env3 = gym.make('Battleships-v0', config=config)\n",
    "env = DummyVecEnv([lambda: env2])\n",
    "env4 = DummyVecEnv([lambda: env3])\n",
    "\n",
    "\n",
    "# Define Callback\n",
    "#Callback stops training if maximum is reached in mean reward\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=env2.calculate_threshold(), verbose=1)\n",
    "# Callback safes the currently best model\n",
    "eval_callback = EvalCallback(env4, callback_on_new_best=callback_on_best, verbose=1, best_model_save_path='./ACKTR_Models/best/')\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1e4, save_path='./model_checkpoints/')\n",
    "\n",
    "# Load current best model\n",
    "model = ACKTR.load(\"./ACKTR_Models/best/best_model.zip\", verbose=2, env=env, tensorboard_log=\"./logs/progress_tensorboard/\")\n",
    "\n",
    "# Uncomment the following line to load a pre trained model\n",
    "#model = ACKTR.load(\"./ACKTR_Models/ACKTR_5x5_3_2_2_Dynamic.zip\", verbose=2, env=env, tensorboard_log=\"./logs/progress_tensorboard/\")\n",
    "\n",
    "# Train model\n",
    "# Only 100.000 for demonstration purposes, otherwise use at least 1.000.000\n",
    "model.learn(100000, callback=[checkpoint_callback, eval_callback])\n",
    "\n",
    "# Delete current model and load the best model\n",
    "del model\n",
    "model = ACKTR.load(\"./ACKTR_Models/best/best_model.zip\", verbose=2, env=env, tensorboard_log=\"./logs/progress_tensorboard/\")\n",
    "\n",
    "\n",
    "\n",
    "# Test trained model\n",
    "results = []\n",
    "for iteration in range(100):\n",
    "    score = 0\n",
    "    print('Iteration', iteration)\n",
    "    # Observed Player board\n",
    "    observation = env.reset()\n",
    "    # Init new Result\n",
    "    result = Result()\n",
    "    done = False\n",
    "    # Amount of moves used to finish the game\n",
    "    rounds = 0\n",
    "    while not done:\n",
    "        rounds += 1\n",
    "        # Get a random action from the action space\n",
    "        # action = env.action_space.sample()\n",
    "        # Agent performs a step\n",
    "        # observation, reward, done, info = env.step(action)\n",
    "        action, _states = model.predict(observation)\n",
    "        nextObservation, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        # Add step to result Object\n",
    "        result.append_history(rounds, action, nextObservation, reward, done, info)\n",
    "        observation = nextObservation\n",
    "        # Game is done\n",
    "        if done:\n",
    "            print(\"End of game: overall_reward=\", result.get_overall_reward(), \",rounds\", rounds, \"score\", score)\n",
    "            # Store amount of rounds in result object\n",
    "            result.set_rounds(rounds)\n",
    "            # Add current result object to all results\n",
    "            results.append(result)\n",
    "print('Finished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mithilfe dieser beiden Trainingsschritte, k√∂nnen die Limitierungen eines festen Action-Spaces von OpenAI Gym \n",
    "in einem annehmbaren Ma√ü behoben werden.  \n",
    "Es w√§re jedoch eine Implementation f√ºr dynamische Action-Spaces seitens OpenAI Gym sinnvoll, sodass  \n",
    "Agenten f√ºr Spiele wie Schiffe versenken nicht doppelt trainiert werden m√ºssten.  \n",
    "Dies w√ºrde eine Menge an Zeit und somit auch Kosten sparen, welche f√ºr das erste Training aufgebracht werden m√ºssen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wie wirken sich unterschiedliche Konfigurationen auf ein Modell aus?\n",
    "Die Umgebung wurde weitesgehend so programmiert, dass sie √ºber eine Konfigurationsklasse in vielerlei Hinsicht konfigurierbar ist. Dies hat den Vorteil, dass die Agenten theoretisch auf viele verschiedene Gegebenheiten trainiert werden k√∂nnen. Zum Beispiel mit unterschiedlicher Spielfeldgr√∂√üe, Anzahl der Schiffe oder die Positionierung der Schiffe. Das Problem bei der momentanen Umsetzung ist, dass der Agent durch den festen Action-Space zweimal trainiert werden muss. So w√§re es auch, wenn sich die Konfiguration der Umgebung √§ndert. Daraus resultiert eine gro√üe Zeitdauer, zum Trainieren der Agenten. Der ACKTR-Agent schneidet hierbei jedoch besser ab, da der Algorithmus mittels stable-baselines auch multi processed trainiert werden kann und so eine h√∂here Rechenleistung zur Verf√ºgung steht.\n",
    "\n",
    "Dar√ºber hinaus hat sich gezeigt, dass die Agenten mit einer Spielfeldgr√∂√üe von 5x5 gut trainiert werden k√∂nnen. Je gr√∂√üer das Feld ist, desto l√§nger dauert das Training und die 1.000.000 Iterationen reichen nicht aus, um ein erfolgreich trainiertes Modell zu erzielen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellen\n",
    "[1] Nick Berry ‚ÄúBattleship‚Äù http://www.datagenetics.com/blog/december32011/  \n",
    "[2] Sue He: 'Deep Reinforcement Learning‚Äìof how to win at Battleship' http://www.ccri.com/2017/08/25/deep-reinforcement-learning-win-battleship/  \n",
    "[3] Fran√ßois-Lavet, Vincent, Peter Henderson, Riashat Islam, Marc G. Bellemare and Joelle Pineau. ‚ÄúAn Introduction to Deep Reinforcement Learning.‚Äù Found. Trends Mach. Learn. 11 (2018): 219-354."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
